"""Embedding layer with Language model

    Some node features are generated by a language model, e.g., BERT
"""

import torch as th
import dgl

from .embed import GSNodeInputLayer
from .lm_model import init_lm_model
from .lm_model import get_lm_node_feats
from ..utils import get_rank

class GSLMNodeInputLayer(GSNodeInputLayer):
    """The input embedding layer with language model for all nodes in a heterogeneous graph.

    The input layer adds language model layer on nodes with textual node features and
    generate LM embeddings using the LM model. The LM embeddings are then treated
    as node features.

    The input layer adds learnable embeddings on nodes if the nodes do not have features.
    It adds a linear layer on nodes with node features and the linear layer
    projects the node features to a specified dimension. A user can add learnable
    embeddings on the nodes with node features. In this case, the input layer
    combines the node features with the learnable embeddings and project them to
    the specified dimension.

    Parameters
    ----------
    g: DistGraph
        The distributed graph
    node_lm_configs:
        A list of language model configurations.
    feat_size : dict of int
        The original feat sizes of each node type
    embed_size : int
        The embedding size
    num_train: int
        Number of trainable texts
    lm_infer_batchszie: int
        Batch size used for computing text embeddings for static lm model
    lm_freeze_epochs: int
        The number of epochs to freeze the lm model while warming-up the rest
        of the network.
    activation : func
        The activation function
    dropout : float
        The dropout parameter
    use_node_embeddings : bool
        Whether we will use the node embeddings for individual nodes even when node features are
        available.
    """
    def __init__(self,
                 g,
                 node_lm_configs,
                 feat_size,
                 embed_size,
                 num_train=0,
                 lm_infer_batchszie=16,
                 lm_freeze_epochs=0,
                 activation=None,
                 dropout=0.0,
                 use_node_embeddings=False):
        assert node_lm_configs is not None and len(node_lm_configs) > 0, \
            "language model configurations must be provided"

        lm_models = th.nn.ModuleList()
        lm_models_info = []
        adjust_feat_size = dict(feat_size)
        for lm_config in node_lm_configs:
            lm_model = init_lm_model(lm_config,
                                     num_train=num_train,
                                     lm_infer_batchszie=lm_infer_batchszie)
            # A list of node types sharing the same lm model
            lm_ntypes = lm_config["node_types"]
            lm_node_feats = get_lm_node_feats(g, lm_model, lm_ntypes)
            lm_models.append(lm_model)
            lm_models_info.append((lm_ntypes, lm_node_feats))

            # Update feature size
            for ntype in lm_ntypes:
                adjust_feat_size[ntype] += lm_model.feat_size
                if get_rank() == 0:
                    print(f'Node {ntype} adds lm {lm_config["lm_type"]} '
                          f'features {feat_size[ntype]}->{adjust_feat_size[ntype]}')

        self.num_train = num_train
        self.lm_infer_batchszie = lm_infer_batchszie
        self.lm_freeze_epochs = lm_freeze_epochs
        self.bert_emb_cache = {}

        super(GSLMNodeInputLayer, self).__init__(
            g, adjust_feat_size, embed_size,
            activation, dropout, use_node_embeddings)

        self.lm_models = lm_models
        self.lm_models_info = lm_models_info

    def _update_bert_cache(self, g):
        for (lm_ntypes, lm_node_feats), lm_model \
            in zip(self.lm_models_info, self.lm_models):
            lm_model.eval()
            for ntype in lm_ntypes:
                if get_rank() == 0:
                    print('compute bert embedding on node {}'.format(ntype))
                hidden_size = lm_model.feat_size
                if 'bert_emb' not in g.nodes[ntype].data:
                    g.nodes[ntype].data['bert_emb'] = \
                        dgl.distributed.DistTensor(
                            (g.number_of_nodes(ntype), hidden_size),
                            name="bert_emb",
                            dtype=th.float32,
                            part_policy=g.get_node_partition_policy(ntype),
                            persistent=True)
                input_emb = g.nodes[ntype].data['bert_emb']
                infer_nodes = dgl.distributed.node_split(
                    th.ones((g.number_of_nodes(ntype),), dtype=th.bool),
                    partition_book=g.get_partition_book(),
                    ntype=ntype, force_even=False)

                node_list = th.split(infer_nodes, self.lm_infer_batchszie)
                input_ntypes = [ntype]
                for _, input_nodes in enumerate(node_list):
                    input_lm_feats = {}
                    input_lm_feats[ntype] = {
                        fname: feat[input_nodes] \
                            for fname, feat in lm_node_feats[ntype].items()
                    }
                    text_embs = lm_model(input_ntypes, input_lm_feats)
                    input_emb[input_nodes] = text_embs[ntype].to('cpu')
                th.distributed.barrier()
                self.bert_emb_cache[ntype] = input_emb
            lm_model.train()

    def warmup(self, g):
        """ Generate Bert caching if needed
        """
        # The bert_emb_cache is used in following cases:
        # 1) We don't need to fine-tune Bert, i.e., train_nodes == 0.
        #    In this case, we only generate bert bert_emb_cache once before model training.
        #
        # 2) GNN warnup when lm_freeze_epochs > 0.
        #    We generate the bert emb_cache before model training.
        #    In the first lm_freeze_epochs epochs, the number of trainable text
        #    nodes are set to 0 and the bert_emb_cache is not refreshed.
        #
        # 3) if train_nodes > 0, no emb_cache is used unless Case 2.
        if self.num_train == 0 or self.lm_freeze_epochs > 0: # it is not initialized elsewhere
            self._update_bert_cache(g)

    #pylint: disable=keyword-arg-before-vararg
    def forward(self, input_feats, input_nodes, epoch=-1, *_):
        """Forward computation

        Parameters
        ----------
        input_feats: dict
            input features
        input_nodes: dict
            input node ids
        epoch: int
            Current training epoch
            Default -1 means epoch is not initialized. (e.g., in prediction)

        Returns
        -------
        a dict of Tensor: the node embeddings.
        """
        assert isinstance(input_feats, dict), 'The input features should be in a dict.'
        assert isinstance(input_nodes, dict), 'The input node IDs should be in a dict.'

        # Compute language model features first
        lm_feats = {}
        if len(self.bert_emb_cache) > 0 and \
            (self.num_train == 0 or 0 <= epoch < self.lm_freeze_epochs):
            # No bert training, Get cached LM embedding
            # Note: self.bert_emb_cache is initialized by calling warmup
            for ntype, idx in input_nodes.items():
                if ntype in self.bert_emb_cache:
                    lm_feats[ntype] = self.bert_emb_cache[ntype][idx]
        else:
            # TODO: Release the bert cache properly
            #       This may need support from DistDGL
            # Need bert training
            for (lm_ntypes, lm_node_feats), lm_model \
                in zip(self.lm_models_info, self.lm_models):
                input_ntypes = []
                input_lm_feats = {}
                for ntype in lm_ntypes:
                    if ntype in input_nodes:
                        input_ntypes.append(ntype)
                        input_lm_feats[ntype] = {
                            fname: feat[input_nodes[ntype]] \
                                for fname, feat in lm_node_feats[ntype].items()
                        }

                if len(input_ntypes) > 0:
                    lm_feats.update(lm_model(input_ntypes, input_lm_feats))

        for ntype, lm_feat in lm_feats.items():
            # move lm_feat to the right device
            # we assume input_feats has already been moved to that device.
            lm_feat = lm_feat.to(next(self.parameters()).device)
            if ntype in input_feats:
                input_feats[ntype] = th.cat((input_feats[ntype].float(), lm_feat), dim=-1)
            else:
                input_feats[ntype] = lm_feat

        return super(GSLMNodeInputLayer, self).forward(input_feats, input_nodes)
