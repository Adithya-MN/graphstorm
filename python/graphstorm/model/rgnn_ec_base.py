"""edge classification based on RGNN
"""

import torch as th
from torch import nn
from torch.nn.parallel import DistributedDataParallel

from .rgnn_edge_base import GSgnnEdgeModel
from .edge_decoder import DenseBiDecoder, MLPEdgeDecoder

class GSgnnEdgeClassificationModel(GSgnnEdgeModel):
    """ RGNN edge classification model

    Parameters
    ----------
    g: DGLGraph
        The graph used in training and testing
    config: GSConfig
        The graphstorm GNN configuration
    task_tracker: GSTaskTrackerAbc
        Task tracker used to log task progress
    train_task: bool
        Whether it is a training task
    """
    def __init__(self, g, config, task_tracker=None, train_task=True):
        super(GSgnnEdgeClassificationModel, self).__init__(g, config, task_tracker, train_task)

        # edge classification related
        self.num_classes = config.num_classes
        # decoder related
        # specify the type of decoder
        self.decoder_type = config.decoder_type
        self.num_decoder_basis = config.num_decoder_basis
        self.multilabel = config.multilabel
        self.multilabel_weights = config.multilabel_weights
        self.imbalance_class_weights = config.imbalance_class_weights

        self.model_conf = {
            'task': 'edge_classification',
            'target_etype': self.target_etype,
            # GNN
            'gnn_model': self.gnn_model_type,
            'num_layers': self.n_layers,
            'hidden_size': self.n_hidden,
            'num_bases': self.n_bases,
            'dropout': self.dropout,
            'use_self_loop': self.use_self_loop,
        }
        # logging all the params of this experiment

    def init_gsgnn_model(self, train=True):
        ''' Initialize the GNN model.

        Argument
        --------
        train : bool
            Indicate whether the model is initialized for training.
        '''
        super(GSgnnEdgeClassificationModel, self).init_gsgnn_model(train)
        loss_func = nn.BCEWithLogitsLoss(pos_weight=self.multilabel_weights) \
             if self.multilabel else \
            nn.CrossEntropyLoss(weight=self.imbalance_class_weights)
        loss_func = loss_func.to(self.dev_id)

        if self.multilabel:
            # BCEWithLogitsLoss wants labels be th.Float
            def lfunc(logits, lbl):
                return loss_func(logits, lbl.type(th.float32))
            self.loss_func = lfunc
        else:
            self.loss_func = loss_func

    def predict(self, logits):
        '''Make prediction on the input logits.

        Parameters
        ----------
        logits : tensor
            The logits generated by the model.

        Returns
        -------
        tensor
            The predicted results.
        '''
        if self.multilabel:
            # multilabel, do nothing
            return logits
        else:
            return logits.argmax(dim=1)

    def init_dist_decoder(self, train):
        dev_id = self.dev_id
        if self.decoder_type == "DenseBiDecoder":
            decoder = DenseBiDecoder(in_units=self.n_hidden,
                                     num_classes=self.num_classes,
                                     num_basis=self.num_decoder_basis,
                                     dropout_rate=self.dropout,
                                     regression=False,
                                     target_etype=self.target_etype)
        elif self.decoder_type == "MLPDecoder":
            decoder = MLPEdgeDecoder(2*self.n_hidden,
                                    self.num_classes,
                                    target_etype=self.target_etype)
        else:
            assert False, "decoder not supported"

        decoder = decoder.to(dev_id)
        # decoder also need to be distributed
        decoder = DistributedDataParallel(decoder,
            device_ids=[dev_id],
            output_device=dev_id,
            find_unused_parameters=True)

        self.decoder = decoder
